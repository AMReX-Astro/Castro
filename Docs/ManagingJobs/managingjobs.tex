Castro requires both a \cpp\ compiler (with support for \cpp\ 11) and
a Fortran compiler with 2003+ support.

\section{General Compilers}

\subsection{GCC}
The GCC compilers are the preferred compilers on Linux machines.
\castro\ runs without issue with GCC 5.3, GCC 6.1, 6.2, and 6.3.

\subsection{PGI}

The PGI compilers (16.4--16.10) are also tested with \castro\ and have
no known issues.  Since PGI uses the GCC header files, a compatible
version of GCC is needed.  This is particularly problematic with \cpp~11
support.  At the moment, with PGI 16.10 compilers the latest GCC compiler
that has compatible \cpp~11 support is GCC 4.9.4.

\section{Working at OLCF (ORNL)}

The build system needs python 2.7+ or 3.5+.  The default python at OLCF
is too old, so you will need to manually load a newer python as:
\begin{verbatim}
module load python
\end{verbatim}

\subsection{Cray}
Note, you will need to swap the default PGI environment to use Cray:
\begin{verbatim}
module swap PrgEnv-pgi PrgEnv-cray
\end{verbatim}

cce/8.5.7 through 8.6.3 fail to compile AMReX. The default version at OLCF as of May 2018,
cce/8.6.4, resolves this issue.

\subsection{PGI}

If you are using OpenACC on the GPUs, then you need to use the PGI
compilers instead.  It is best to use the latest available PGI
compilers, which are 17.7 at this writing.  
\begin{verbatim}
module swap pgi pgi/17.7
\end{verbatim}

These seem to build the code without problems.

This is perhaps out of date:
Finally, to use OpenACC, you need to make the CUDA libraries available to the PGI compiler:
\begin{verbatim}
module load craype-accel-nvidia35
\end{verbatim}

\subsection{Automatic Restarting and Archiving of Data}

See the \maestro\ User's Guide


\section{Working at NERSC}

\subsection{General issues}

%%\begin{itemize}

%% \item 3/1/16: The default version loaded of Python on Edison and Cori
%%   (2.6.9) is not recent enough to support the Python scripts in our
%%   build system. At the terminal, do {\tt module load python} to fix
%%   this.

%%   Note: if you want to swap from the default Intel compilers to the 
%%   Cray compiler, this will break the python installation.  The workaround
%%   is to do:
%%   \begin{verbatim}
%%     $ module rm PrgEnv-intel
%%     $ module load PrgEnv-cray
%%   \end{verbatim}

%%\item 3/1/16: The default Intel Compiler loaded on Cori (16.0.0.109)
%  cannot compile BoxLib. To fix this, switch to an earlier version
%  (e.g., at the terminal, do {\tt module swap intel
%    intel/15.0.1.133}).

%\end{itemize}

\subsection{Edison compilers}

\begin{itemize}
\item Intel compilers

  The default compilers on edison are the Intel compilers.  

  The Intel 17.0.2 compilers produce code that crashes at runtime when self-gravity
  is used (the error is in the {\tt F\_MG} directory.  This appears to be an issue since
  16.0.2.

  At the moment, avoid Intel compilers on Edison.

%% \item To compile the code with Intel compilers, you need to explicitly load GCC 4.9.3,
%% e.g. as:
%% \begin{verbatim}
%% module load gcc/4.9.3
%% \end{verbatim}
%% Otherwise you will get compilation errors due to incompatibilities in the header files.

\item Cray compilers

  \begin{itemize}

    \item The Cray compilers from versions 8.5.7 to 8.6.3 have issues compiling AMReX. This
    can be resolved by loading version 8.6.4 or later. As of May 2018, you can do
    \begin{verbatim}
      module swap cce cce/8.6.5
    \end{verbatim}
   \end{itemize}
\item GNU

  The GNU compilers (6.3) seem to work fine.

\end{itemize}


\subsection{Cori (KNL) compilers}

To compile with Cray compilers on Cori, we need to swap the compiler
wrappers to use the AVX-512 instruction set supported on the Intel Phi
processors instead of the AVX-2 extensions used by the Intel Haswell
architecture.  This is done as:
\begin{verbatim}
module swap craype-{haswell,mic-knl}
\end{verbatim}

It could happen that even when the various verbosities are set to 0, when using several nodes (more than 64) in a run compiled with Intel, the follwing error shows:
\begin{verbatim}
"forrtl: severe (40): recursive I/O operation, unit -1, file unknown"
\end{verbatim}

Seems like the error is due to all threads printing to stdout. Adding the following to the inputs file, prevents this error to occur:
\begin{verbatim}
castro.print_fortran_warnings = 0
\end{verbatim}


\subsection{Hypre and radiation}

On edison, the Cray {\em Third Party Scientific Libraries} provide
{\sf hypre} in a form that works directly with the compiler wrappers
used on that machine ({\tt CC}, {\tt ftn}, $\ldots$).  To use this,
simply do:
\begin{verbatim}
module load cray-tpsl
\end{verbatim}
There is no need to set {\tt HYPRE\_DIR}, but note however that the 
dependency checker script ({\tt BoxLib/Tools/C\_scripts/mkdep}) will
complain about:
\begin{verbatim}
/path/to/Hypre--with-openmp/include does not exist
\end{verbatim}
This can be ignored an compilation will finish.  If you do wish to 
silence it, you can set {\tt HYPRE\_DIR} to the path shown by
\begin{verbatim}
module show cray-tpsl
\end{verbatim}
as
\begin{verbatim}
export HYPRE_DIR=${CRAY_TPSL_PREFIX_DIR}
\end{verbatim}
This path will change dynamically to reflect which compiler programming
environment you have loaded.  (You can also see that this is the path
sent to the compilation by doing {\tt ftn -craype-verbose}).


\subsection{Running jobs}

edison is configured with 24 cores per node split between two Intel             
IvyBridge 12-core processors.  Each processor connects to 1/2 of the            
node's memory and is called a NUMA node, so there are 2 NUMA nodes per          
edison node.  Best performance is seen when running with 6 or 12 threads.  

Jobs should be run in your {\tt \$SCRATCH} or {\tt \$CSCATCH} directory.
By default, SLURM will change directory into the submission directory.

A sample job submission script, {\tt edison.MPI.OMP.slurm} is in {\tt
  Castro/Util/job\_scripts/edison/}, and includes logic to
automatically add the correct restart options to the run to continue a
simulation from the last checkpoint file in the submission directory.

To chain jobs, such that one queues up after the previous job finished,         
use the {\tt chainslurm.sh} script in that same directory:
\begin{verbatim}
chainslurm.sh jobid number script
\end{verbatim}
where {\tt jobid} is the existing job you want to start you chain
from, {\tt number} is the number of new jobs to chain from this
starting job, and {\tt script} is the job submission script to use
(the same one you used originally most likely).  You can view the job
dependency using:
\begin{verbatim}                                                                
squeue -l -j job-id                                                             
\end{verbatim}                                                                  
where {\tt job-id} is the number of the job.                                    
                                                                                
Jobs are submitted with {\tt sbatch}.  A job can be canceled using              
{\tt scancel}, and the status can be checked using {\tt squeue -u {\em          
username}}.                                                                     


\subsection{Archiving data to HPSS}

The script {\tt edison.xfer.slurm} in {\tt
  Castro/Util/job\_scripts/edison/} can be used to archive data to
HPSS automatically.  This is submitted to the {\tt xfer} queue and
runs the script {\tt process.xrb} which continually looks for output
and stores it to HPSS.

To use the scripts, first create a directory in HPSS that has the same          
name as the directory on lustre you are running in (just the directory          
name, not the full path).  E.g.\ if you are running in a directory              
call {\tt wdconvect\_run}, then do:                                             
\begin{verbatim}                                                                
hsi                                                                             
mkdir wdconvect_run                                                             
\end{verbatim}                                                                  
(Note: if the hsi command prompts you for your password, you will need          
to talk to the NERSC help desk to ask for password-less access to                
HPSS).                                                                          
                                                                                
The script {\tt process.xrb} is called from the xfer job and will
run in the background and continually wait until checkpoint or                  
plotfiles are created (actually, it always leaves the most recent one           
alone, since data may still be written to it, so it waits until there           
are more than 1 in the directory).                                              
                                                                                
Then the script will use {\tt htar} to archive the plotfiles and                
checkpoints to HPSS.  If the {\tt htar} command was successful, then            
the plotfiles are copied into a {\tt plotfile/} subdirectory.  This is          
actually important, since you don't want to try archiving the data a            
second time and overwriting the stored copy, especially if a purge              
took place.  The same is done with checkpoint files.
                                                                                
Additionally, if the {\tt ftime} executable is in your path ({\tt               
ftime.f90} lives in {\tt BoxLib/Tools/Postprocessing/F\_src/}), then            
the script will create a file called {\tt ftime.out} that lists the             
name of the plotfile and the corresponding simulation time.                     
                                                                                
Finally, right when the job is submitted, the script will tar up all
of the diagnostic files, {\tt ftime.out}, submission script, inputs
and probin, and archive them on HPSS.  The {\tt .tar} file is given a
name that contains the date-string to allow multiple archives to
co-exist.
                                                                    
When {\tt process.xrb} is running, it creates a lockfile (called              
{\tt process.pid}) that ensures that only one instance of the script            
is running at any one time.  Sometimes if the machine crashes, the              
{\tt process.pid} file will be left behind, in which case, the script           
aborts.  Just delete that if you know the script is not running.   

Jobs in the {\tt xfer} queue start up quickly.  The best approach is            
to start one as you start your main job (or make it dependent on the            
main job).  The sample {\tt process.xrb} script will wait for output            
and then archive it as it is produced, using the techniques described           
for titan above.                                                                
                                                                                
To check the status of a job in the {\tt xfer} queue, use:                      
\begin{verbatim}                                                                
squeue -u username -M all                                                       
\end{verbatim}           



\section{Working at LANL}

For the following LANL systems, which all have access to a joint file system,
\begin{itemize}
  \item Cielito
  \item Conejo
  \item Lightshow
  \item Moonlight
  \item Pinto
  \item Wolf
  \item Mustang
  \item Trinitite
\end{itemize}
the following steps are needed to get \castro\ compiling (reported by Platon Karpov, 6/13/2016):

1) Compile on the login node (thus host is lanl.gov)\\
2) Do \textit{not} set env variable \texttt{BOXLIB\_USE\_MPI\_WRAPPERS} at all\\
3) Execute \texttt{module load python-epd} at the command line, but do \textit{not} load anaconda\\


\section{Scaling}

Data from scaling studies is archived in {\tt Castro/Docs/ManagingJobs/scaling/}

Needs to be updated



\section{Gotyas}

\begin{enumerate}

\item 3/4/16: The default version loaded of Python on Mira is not
  recent enough to support the Python scripts in our build system. Add
  {\tt +python} to your {\tt .soft} to fix this.

\item 2/18/16: The default version loaded of Python on Titan (2.6.9)
  is not recent enough to support the Python scripts in our build
  system. At the terminal, do {\tt module load python} to fix this.


\end{enumerate}



\section{GPUs}

\section{Bender}
Compile as:
\begin{verbatim}
make CUDA_VERSION=cc60 COMPILE_CUDA_PATH=/usr/local/cuda-9.2 USE_CUDA=TRUE COMP=PGI -j 4
\end{verbatim}

To run the CUDA code path without device launching, do:
\begin{verbatim}
make -j4 COMP=PGI USE_CUDA=TRUE USE_MPI=FALSE DEBUG=TRUE NO_DEVICE_LAUNCH=TRUE CUDA_VERSION=cc60 COMPILE_CUDA_PATH=/usr/local/cuda-9.2
\end{verbatim}

