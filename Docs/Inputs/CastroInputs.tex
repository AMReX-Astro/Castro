The \castro\ executable uses two inputs files at runtime to set and alter the
behavior of the algorithm and initial conditions.

The main inputs file, typically named {\tt inputs} is used to
set \boxlib\ parameters and the control flow in the \cpp\ portions of
the \castro\ code.  Each parameter here has a namespace (like {\tt
amr.{\em optionname}} or {\tt castro.{\em optionname}}).  Parameters
set here are read using the \boxlib\ {\tt ParmParse} class
infrastructure.

The second inputs file, typically named \runparam{probin} is used by
the Fortran code that initializes the problem setup.  It is read at
problem initialization (via a Fortran {\tt namelist}) and the
problem-specific quantities are stored in a Fortran module
\code{probdata\_module} defined in the problem's {\tt probdata.f90}
file.

Only the {\tt inputs} file is specified on the commandline.  The
associated {\tt probin} file is specified in the {\tt inputs} file
using the {\tt amr.probin\_file} parameter, e.g.,
\begin{lstlisting}
amr.probin_file = my_special_probin
\end{lstlisting}
for example, has the Fortran code read a file called {\tt my\_special\_probin}.

\section{Working with {\tt probin} Files}

There are three different Fortran {\tt namelist}s that can be defined in the
{\tt probin} file:
\begin{itemize}
\item {\tt \&fortin} is the main {\tt namelist} read by the problem's {\tt probinit}
subroutine in the {\tt Prob\_?d.f90} file.

\item {\tt \&extern} is used to set different microphysics options

\item {\tt \&tagging} is used to get the parameters (defined in \code{tagging\_module})
that affect how we tag for refinement.
\end{itemize}

\section{Common {\tt inputs} Options}

{\bf Important}: because the {\tt inputs} file is handled by the \cpp\ portion of
the code, any quantities you specify in scientific notation, must take the
form {\tt 1.e5} and not {\tt 1.d5}---the `{\tt d}' specifier is not recognized.

Additionally, note that in \castro, all quantities are in CGS units.

\subsection{Problem Geometry}

The {\tt geometry} namespace is used by \boxlib\ to define the
computational domain.  The main parameters here are:
\begin{itemize}
\item \runparam{geometry.prob\_lo}: physical location of low corner of the
domain (type: {\tt Real}; must be set)

  Note: a number is needed for each dimension in the problem
  
\item \runparam{geometry.prob\_hi}: physical location of high corner of the
domain (type: {\tt Real}; must be set)

  Note: a number is needed for each dimension in the problem
  
\item \runparam{geometry.coord\_sys}: coordinate system, 0 = Cartesian,
1 = $r$-$z$ (2-d only), 2 = spherical (1-d only) (must be set)

\item \runparam{geometry.is\_periodic}: is the domain periodic in this direction?
  {\tt 0} if false, {\tt 1} if true  (default: {\tt 0 0 0}) 

  Note: an integer is needed for each dimension in the problem

\item {\tt castro.center}: physical location of problem center on the
  domain (type: {\tt Real}; default: {\tt 0.0 0.0 0.0}).  The problem
  center is used for gravity, rotation, and some other quantities.
  This is not necessarily the geometric center of the domain---often
  you should choose it to coincide with the center of mass of your
  system.  See \S~\ref{soft:prob_params} for more details.

  Note: a number is needed for each dimension in the problem

\end{itemize}

As an example, the following:
\begin{lstlisting}
geometry.prob_lo = 0 0 0
geometry.prob_hi = 1.e8 2.e8 2.e8 
geometry.coord_sys = 0 
geometry.is_periodic = 0 1 0 
castro.center = 5.e7 1.e8 1.e8
\end{lstlisting}

This defines the domain to run from $(0,0,0)$ at the lower left to
$(10^8,\, 2\times 10^8,\, 2\times 10^8)$ at the upper right in physical
space, specifies a Cartesian geometry, and makes the domain periodic
in the $y$-direction only. The problem center is set to be halfway in
between the lower left and upper right corners.


\subsection{Domain Boundary Conditions}

Boundary conditions are specified using integer keys that are interpreted
by \boxlib.  The runtime parameters that we use are:
\begin{itemize}
\item {\tt castro.lo\_bc}: boundary type of each low face  (must be set)
\item {\tt castro.hi\_bc}: boundary type of each high face (must be set)
\end{itemize}

The valid boundary types are:
\begin{table*}[h]
\begin{center}
\begin{tabular}{llll} 
{\tt 0} --  Interior / Periodic \hspace{1.in} & {\tt 3}  --  Symmetry     \hspace{1.in} & \\
{\tt 1} --  Inflow              \hspace{1.in} & {\tt 4}  --  Slip Wall    \hspace{1.in}& \\
{\tt 2} --  Outflow             \hspace{1.in} & {\tt 5}  --  No Slip Wall \hspace{1.in}& \\
\end{tabular}
\end{center}
\end{table*}

\noindent Note: {\tt castro.lo\_bc} and {\tt castro.hi\_bc} must be
consistent with {\tt geometry.is\_periodic}---if the domain is
periodic in a particular direction then the low and high bc's must be
set to {\tt 0} for that direction.

As an example, the following:
\begin{lstlisting}
castro.lo_bc = 1 4 0 
castro.hi_bc = 2 4 0 

geometry.is_periodic = 0 0 1
\end{lstlisting}

This defines a problem with inflow ({\tt 1}) in the low-$x$ direction,
outflow ({\tt 2}) in the high-$x$ direction, slip wall ({\tt 4}) on
the low and high $y$-faces, and periodic in the $z$-direction.
See \S~\ref{soft:phys_bcs} for more information.

\subsection{Resolution}

The grid resolution is specified by defining the resolution at the
coarsest level (level 0) and the number of refinement levels and
factor of refinement between levels.  The relevant parameters are:
\begin{itemize}
\item \runparam{amr.n\_cell}: number of cells in each direction at the
  coarsest level (Integer $> 0$; must be set)

\item \runparam{amr.max\_level}: number of levels of refinement above the
  coarsest level (Integer $\geq 0$; must be set)

\item \runparam{amr.ref\_ratio}: ratio of coarse to fine grid spacing
  between subsequent levels (2 or 4; must be set)

\item \runparam{amr.regrid\_int}: how often (in terms of number of steps)
  to regrid (Integer; must be set)

\item \runparam{amr.regrid\_on\_restart}: should we regrid immediately
  after restarting? (0 or 1; default: 0)
\end{itemize}

Note: if {\tt amr.max\_level = 0} then you do not need to set {\tt
  amr.ref\_ratio} or {\tt amr.regrid\_int}.

Some examples:
\begin{lstlisting}
amr.n_cell = 32 64 64
\end{lstlisting}
would define the domain to have 32 cells in the $x$-direction, 64 cells
in the $y$-direction, and 64 cells in the $z$-direction {\em{at the
coarsest level}}.  (If this line appears in a 2D inputs file then the
final number will be ignored.)\newline

\begin{lstlisting}
amr.max_level = 2 
\end{lstlisting}
would allow a maximum of 2 refined levels in addition to the coarse
level.  Note that these additional levels will only be created only if
the tagging criteria are such that cells are flagged as needing
refinement.  The number of refined levels in a calculation must be
$\leq$ {\tt amr.max\_level}, but can change in time and need not
always be equal to {\tt amr.max\_level}.  \newline
 
\begin{lstlisting}
amr.ref_ratio = 2 4 
\end{lstlisting}
would set factor of 2 refinement between levels 0 and 1, and factor of 4
refinement between levels 1 and 2.  Note that you must have at least
{\tt amr.max\_level} values of {\tt amr.ref\_ratio} (Additional values
may appear in that line and they will be ignored). \newline

\begin{lstlisting}
amr.regrid_int = 2 2
\end{lstlisting}
tells the code to regrid every 2 steps.  Thus in this example, new
level 1 grids will be created every 2 level-0 time steps, and new
level 2 grids will be created every 2 level-1 time steps. If {\tt
amr.regrid\_int} $<$ 0 for any level, then regridding starting at that
level will be disabled. If {\tt amr.regrid\_int = -1} only, then we
never regrid for any level. Note that this is not compatible with {\tt
amr.regrid\_on\_restart = 1}.


\subsection{Regridding}

The details of the regridding strategy are described in
\S~\ref{sec:tagging}; here we cover how the input parameters can
control the gridding.

As described later, the user defines Fortran subroutines which tag
individual cells at a given level if they need refinement.  This list
of tagged cells is sent to a grid generation routine, which uses the
Berger-Rigoutsos algorithm~\cite{br-refine} to create rectangular
grids that contain the tagged cells.

The relevant runtime parameters are:
\begin{itemize}
\item \runparam{amr.regrid\_file}: name of file from which to read the
  grids (text; default: no file)

  If set to a filename, e.g.\ {\tt fixed\_girds}, then list of grids
  at each fine level are read in from this file during the gridding
  procedure. These grids must not violate the {\tt
  amr.max\_grid\_size} criterion.  The rest of the gridding procedure
  described below will not occur if {\tt amr.regrid\_file} is set.

\item \runparam{amr.grid\_eff}: grid efficiency (Real $>0$ and $<1$;
  default: 0.7)

\item \runparam{amr.n\_error\_buf}: radius of additional tagging
  around already tagged cells (Integer $\geq 0$; default: 1)

\item \runparam{amr.max\_grid\_size}: maximum size of a grid in any
  direction (Integer $> 0$; default: 128 (2-d), 32 (3-d))

   Note: {\tt amr.max\_grid\_size} must be even, and a multiple of
   {\tt amr.blocking\_factor} at every level.
   
\item \runparam{amr.blocking\_factor}: grid size must be a multiple of this
  (Integer $> 0$; default: 2)

   Note: {\tt amr.blocking\_factor} at every level must be a power of
   2 and the domain size must be a multiple of {\tt
     amr.blocking\_factor} at level 0.
   
   This can be very important for elliptic problems with
   multigrid\index{multigrid}.  A higher blocking factor allows the
   multigrid algorithm to coarsen more at the lowest level, reducing
   the amount of work required by the bottom solver.

\item \runparam{amr.refine\_grid\_layout}: refine grids more if \# of
  processors $>$ \# of grids (0 if false, 1 if true; default: 1) \\
\end{itemize}

Note also that {\tt amr.n\_error\_buf}, {\tt amr.max\_grid\_size} and
{\tt amr.blocking\_factor} can be read in as a single value which is
assigned to every level, or as multiple values, one for each level.

As an example, consider:
\begin{lstlisting}
amr.grid_eff = 0.9
amr.max_grid_size = 64 
amr.blocking_factor} = 32
\end{lstlisting}

The grid efficiency, {\tt amr.grid\_eff}, means that during the grid
creation process, at least 90\% of the cells in each grid at the level
at which the grid creation occurs must be tagged cells.  A higher
grid efficiency means fewer cells at higher levels, but may result
in the production of lots of small grids, which have inefficient cache
and OpenMP performance and higher communication costs.

The {\tt amr.max\_grid\_size} parameter means that the final grids
will be no longer than 64 cells on a side at every level.
Alternately, we could specify a value for each level of refinement as:
{\tt amr.max\_grid\_size = 64 32 16}, in which case our final grids
will be no longer than 64 cells on a side at level 0, 32 cells on a
side at level 1, and 16 cells on a side at level 2.  The {\tt amr.blocking\_factor}
means that all of the final grids will be multiples of 32 at all levels.
Again, this can be specified on a level-by-level basis, like
{\tt amr.blocking\_factor = 32 16 8}, in which case the 
dimensions of all the final grids will be multiples of 32
at level 0, multiples of 16 at level 1, and multiples of 8 at level 2.

\subsubsection{Getting good performance}

These parameters can have a large impact on the performance
of \castro, so taking the time to experiment with is worth the effort.
Having grids that are large enough to coarsen multiple levels in a
V-cycle is essential for good multigrid performance in simulations
that use self-gravity.

{\color{red} Need more experience here}

\subsubsection{How grids are created}

The gridding algorithm proceeds in this order:
\begin{enumerate}
\item Grids are created using the Berger-Rigoutsos clustering algorithm 
modified to ensure that all new fine grids are divisible by {\tt
amr.blocking\_factor}.

\item Next, the grid list is chopped up if any grids are larger than {\tt max\_grid\_size}.
Note that because {\tt amr.max\_grid\_size} is a multiple of {\tt
amr.blocking\_factor} the {\tt amr.blocking\_factor} criterion is
still satisfied.

\item Next, if {\tt amr.refine\_grid\_layout = 1} and there are more processors than grids, and
if {\tt amr.max\_grid\_size} / 2 is a multiple of {\tt amr.blocking\_factor},
then the grids will be redefined, at each level independently, so that
the maximum length of a grid at level $\ell$, in any dimension, is
{\tt amr.max\_grid\_size}[$\ell$] / 2.

\item Finally, if {\tt amr.refine\_grid\_layout = 1},  and there are still more processors
than grids, and if {\tt amr.max\_grid\_size} / 4 is a multiple of {\tt
amr.blocking\_factor}, then the grids will be redefined, at each level
independently, so that the maximum length of a grid at level $\ell$,
in any dimension, is {\tt amr.max\_grid\_size}[$\ell$] / 4.
\end{enumerate}


\subsection{Simulation Time}

There are two paramters that can define when a simulation ends:
\begin{itemize}
\item {\tt max\_step}: maximum number of level 0 time steps (Integer $\geq 0$; default: -1)
\item {\tt stop\_time}: final simulation time (Real $\geq 0$;  default: -1.0)
\end{itemize}
To control the number of time steps, you can limit by the maximum
number of level 0 time steps ({\tt max\_step}) or by the final
simulation time ({\tt stop\_time}), or both. The code will stop at
whichever criterion comes first.

Note that if the code reaches {\tt stop\_time} then the final time
step will be shortened so as to end exactly at {\tt stop\_time}, not
past it.

As an example: 
\begin{lstlisting}
max_step  = 1000
stop_time  = 1.0
\end{lstlisting}
will end the calculation when either the simulation time reaches 1.0 or 
the number of level 0 steps taken equals 1000, whichever comes first.


\subsection{Time Step}
If {\tt castro.do\_hydro = 1}, then typically 
the code chooses a time step based on the CFL number:
\begin{equation}
dt = \mathtt{cfl} \frac{\Delta x}{\max_{i,j,k}\{|u|_{i,j,k}+c_{i,j,k}\}}
\label{eq:cfl}
\end{equation}

The following parameters affect the timestep choice:
\begin{itemize}
\item {\tt castro.cfl}: CFL number (Real $> 0$ and $\leq 1$; default: 0.8)

\item {\tt castro.init\_shrink}: factor by which to shrink the initial
   time step (Real $> 0$ and $\leq 1$; default: 1.0)

\item {\tt castro.change\_max}: factor by which the time step can grow in
   subsequent steps (Real $\geq 1$; default: 1.1)

\item {\tt castro.fixed\_dt}: level 0 time step regardless of cfl or other settings
   (Real $> 0$; unused if not set)

\item {\tt castro.initial\_dt}: initial level 0 time
   step regardless of other settings (Real $> 0$;  unused if not set)

\item {\tt castro.dt\_cutoff}: time step below which calculation will abort
   (Real $> 0$; default: 0.0)

\item {\tt castro.hard\_cfl\_limit}: whether or not to abort the simulation if the hydrodynamics
   update creates velocities that violate the CFL criterion (Integer; default: 1)
\end{itemize}

As an example, consider:
\begin{lstlisting}
castro.cfl = 0.9 
castro.init_shrink = 0.01 
castro.change_max = 1.1
castro.dt_cutoff = 1.e-20
\end{lstlisting}
This defines the $\mathtt{cfl}$ parameter in Eq.~\ref{eq:cfl} to be 0.9,
but sets (via {\tt init\_shrink}) the first timestep we take
to be 1\% of what it would be otherwise.  This allows us to
ramp up to the hydrodynamic timestep at the start of a simulation.
The {\tt change\_max} parameter restricts the timestep from increasing
by more than 10\% over a coarse timestep.    Note that the time step
can shrink by any factor; this only controls the extent to which it can grow.
The {\tt dt\_cutoff} parameter will force the code to abort if the
timestep ever drops below $10^{-20}$.  This is a safety feature---if the
code hits such a small value, then something likely went wrong in the
simulation, and by aborting, you won't burn through your entire allocation
before noticing that there is an issue.

If we know what we are doing, then we can force a particular timestep:
\begin{lstlisting}
castro.fixed_dt = 1.e-4
\end{lstlisting}
sets the level 0 time step to be 1.e-4 for the entire simulation,
ignoring the other timestep controls.  Note that if {\tt
castro.init\_shrink} $\neq 1$ then the first time step will in fact be
{\tt castro.init\_shrink} $\cdot$ {\tt castro.fixed\_dt}.
\begin{lstlisting}
castro.initial_dt = 1.e-4
\end{lstlisting}
sets the {\it initial} level 0 time step to be $10^{-4}$ regardless of 
{\tt castro.cfl} or {\tt castro.fixed\_dt}.  The time step can
grow in subsequent steps by a factor of {\tt castro.change\_max} each step.


\subsection{Subcycling}
Castro supports a number of different modes for subcycling in time.

\begin{itemize}
\item If {\tt amr.subcycling\_mode = Auto} (default), then the code 
will run with equal refinement in space and time. In other words, if 
level $n+1$ is a factor of 2 refinement above level $n$, then $n+1$ 
will take 2 steps of half the duration for every level $n$ step.

\item If {\tt amr.subcycling\_mode = None}, then the code 
will not refine in time. All levels will advance together with a 
timestep dictated by the level with the strictest $dt$. Note that this 
is identical to the deprecated command {\tt amr.nosub = 1}.

\item If {\tt amr.subcycling\_mode = Manual}, then the code will 
subcycle according to the values supplied by {\tt 
subcycling\_iterations}.

\end{itemize}

In the case of {\tt amr.subcycling\_mode = Manual},
we subcycle in manual mode with largest allowable timestep.  The
number of iterations at each level is then specified as:
\begin{lstlisting}
amr.subcycling_iterations = 1 2 1 2
\end{lstlisting}
Here, we take 1 level-0 timestep at a time (required). Take 2 level-1
timesteps for each level-0 step, 1 timestep at level-2 for each
level-1 step, and take 2 timesteps at level-3 for each level-2 step.
Alternately, we could do:
\begin{lstlisting}
amr.subcycling_iterations = 2
\end{lstlisting}
which will subcycle twice at every level (except level 0).


\subsection{Restart Capability}

\castro\ has a standard sort of checkpointing and restarting capability. 
In the inputs file, the following options control the generation of
checkpoint files (which are really directories):
\begin{itemize}
\item {\tt amr.check\_file}: prefix for restart files (text; default: {\tt chk}) 

\item {\tt amr.check\_int}: how often (by level 0 time steps) to write
   restart files (Integer $> 0$; default: -1)

\item {\tt amr.check\_per}: how often (by simulation time) to
  write restart files (Real $> 0$; default: -1.0)

  Note that {\tt amr.check\_per} will write a checkpoint at the first
  timestep whose ending time is past an integer multiple of this interval.
  In particular, the timestep is not modified to match this interval, so
  you won't get a checkpoint at exactly the time you requested.

\item {\tt amr.restart}: name of the file (directory) from which to restart
  (Text; not used if not set)

\item {\tt amr.checkpoint\_files\_output}: should we write checkpoint files? (0 or 1; default: 1)

  If you are doing a scaling study then set {\tt
  amr.checkpoint\_files\_output = 0} so you can test scaling of the
  algorithm without I/O.

\item {\tt amr.check\_nfiles}: how parallel is the writing of the checkpoint files?
  (Integer $\geq 1$; default: 64)

  See the Software Section for more details on parallel I/O and the 
  {\tt amr.check\_nfiles} parameter.

\item {\tt amr.checkpoint\_on\_restart}: should we write a checkpoint immediately after restarting?
  (0 or 1; default: 0)

\item {\tt castro.grown\_factor}: factor by which domain has been grown
  (Integer $\geq 1$; default: 1)
\end{itemize}


Note:
\begin{itemize}

\item You can specify both {\tt amr.check\_int} or {\tt amr.check\_per},
  if you so desire; the code will print a warning in case you did this
  unintentionally. It will work as you would expect -- you will get checkpoints
  at integer multiples of {\tt amr.check\_int} timesteps and at integer
  multiples of {\tt amr.check\_per} simulation time intervals.

\item {\tt amr.plotfile\_on\_restart} and {\tt amr.checkpoint\_on\_restart} 
only take effect if {\tt amr.regrid\_on\_restart} is in effect.
\end{itemize}

As an example,
\begin{lstlisting}
amr.check_file = chk_run
amr.check_int = 10
\end{lstlisting}
means that restart files (really directories) starting with the prefix
``{\tt chk\_run}'' will be generated every 10 level-0 time steps.  The
directory names will be {\tt chk\_run00000}, {\tt chk\_run00010}, {\tt
chk\_run00020}, etc.

If instead you specify
\begin{lstlisting}
amr.check_file = chk_run
amr.check_per = 0.5
\end{lstlisting}
then restart files (really directories) starting with the prefix
``{\tt chk\_run}'' will be generated every {\tt 0.1} units of
simulation time.  The directory names will be {\tt chk\_run00000},
{\tt chk\_run00043}, {\tt chk\_run00061}, etc, where $t = 0.1$ after
43 level-0 steps, $t = 0.2$ after 61 level-0 steps, etc.


To restart from {\tt chk\_run00061}, for example, then set 
\begin{lstlisting}
amr.restart = chk_run00061
\end{lstlisting}


\subsection{Controlling Plotfile Generation}
\label{sec:PlotFiles}
The main output from \castro\ is in the form of plotfiles (which are
really directories).  The following options in the inputs file control
the generation of plotfiles:
\begin{itemize}
\item {\tt amr.plot\_file}: prefix for plotfiles (text; default:
  ``{\tt plt}'')

\item {\tt amr.plot\_int}: how often (by level-0 time steps) to write
  plot files (Integer $> 0$; default: -1)

\item {\tt amr.plot\_per}: how often (by simulation time) to write
  plot files (Real $> 0$; default: -1.0)

  Note that {\tt amr.plot\_per} will write a plotfile at the first
  timestep whose ending time is past an integer multiple of this interval.
  In particular, the timestep is not modified to match this interval, so
  you won't get a checkpoint at exactly the time you requested.

\item {\tt amr.plot\_vars}: name of state variables to include in
  plotfiles (valid options: {\tt ALL}, {\tt NONE} or a list; default:
  {\tt ALL})

\item {\tt amr.derive\_plot\_vars}: name of derived variables to
  include in plotfiles (valid options: {\tt ALL}, {\tt NONE} or a
  list; default: {\tt NONE}

\item {\tt amr.plot\_files\_output}: should we write plot files? (0 or
  1; default: 1)

If you are doing a scaling study then set {\tt amr.plot\_files\_output
  = 0} so you can test scaling of the algorithm without I/O.

\item {\tt amr.plotfile\_on\_restart}: should we write a plotfile
  immediately after restarting?  (0 or 1; default: 0)
  
\item {\tt amr.plot\_nfiles}: how parallel is the writing of the
  plotfiles?  (Integer $\geq 1$; default: 64)

See the Software Section for more details on parallel I/O and the {\tt
  amr.plot\_nfiles} parameter.

\item {\tt castro.plot\_X}: include all the species mass fractions in
  the plotfile (0 or 1; default: 0)

\end{itemize}

All the options for {\tt amr.derive\_plot\_vars} are kept in
\texttt{derive\_lst} in {\tt Castro\_setup.cpp}.  Feel free to look at
it and see what's there.

{\color{red} discuss extra derives options}

Some notes:
\begin{itemize}

\item You can specify both {\tt amr.plot\_int} or {\tt amr.plot\_per},
  if you so desire; the code will print a warning in case you did this
  unintentionally. It will work as you would expect -- you will get plotfiles
  at integer multiples of {\tt amr.plot\_int} timesteps and at integer
  multiples of {\tt amr.plot\_per} simulation time intervals.

\end{itemize}


As an example:
\begin{lstlisting}
amr.plot_file = plt_run
amr.plot_int = 10
\end{lstlisting}
means that plot files (really directories) starting with the prefix
``{\tt plt\_run}'' will be generated every 10 level-0 time steps.  The
directory names will be {\tt plt\_run00000}, {\tt plt\_run00010}, {\tt
  plt\_run00020}, etc.


If instead you specify
\begin{lstlisting}
amr.plot_file = plt_run
amr.plot_per = 0.5
\end{lstlisting}
then restart files (really directories) starting with the prefix
``{\tt plt\_run}'' will be generated every 0.1 units of simulation time.  The
directory names will be {\tt plt\_run00000}, {\tt plt\_run00043}, {\tt
  plt\_run00061}, etc, where $t = 0.1$ after 43 level-0 steps, $t =
0.2$ after 61 level-0 steps, etc.



\subsection{Screen Output}

There are several options that set how much output is written to the
screen as \castro\ runs:
\begin{itemize}
\item {\tt amr.v}: verbosity of {\tt Amr.cpp} (0 or 1; default: 0)

\item {\tt castro.v}: verbosity of {\tt Castro.cpp} (0 or 1; default: 0)

\item {\tt gravity.v}: verbosity of {\tt Gravity.cpp} (0 or 1; default: 0)
  
\item {\tt diffusion.v}: verbosity of {\tt Diffusion.cpp} (0 or 1;
  default: 0)
  
\item {\tt mg.v}: verbosity of multigrid solver (for gravity) (allow
  values: 0,1,2,3,4; default: 0)
  
\item {\tt amr.grid\_log}: name of the file to which the grids are
  written (text; not used if not set)
  
\item {\tt amr.run\_log}: name of the file to which certain output is
  written (text; not used if not set)
  
\item {\tt amr.run\_log\_terse}: name of the file to which certain
  (terser) output is written (text; not used if not set)
  
\item {\tt amr.sum\_interval}: if $> 0$, how often (in level-0 time
  steps) to compute and print integral quantities (Integer; default: -1)

  The integral quantities include total mass, momentum and energy in
  the domain every {\tt castro.sum\_interval} level-0 steps.
  The print statements have the form
  \begin{verbatim}
    TIME= 1.91717746 MASS= 1.792410279e+34
  \end{verbatim}
  for example.  If this line is commented out then
  it will not compute and print these quanitities.

  
\item {\tt castro.do\_special\_tagging}: allows the user to set a
  special flag based on user-specified criteria (0 or 1; default: 1)

  {\tt castro.do\_special\_tagging = 1} can be used, for example, to
  calculate the bounce time in a core collapse simulation; the bounce
  time is defined as the first time at which the maximum density in
  the domain exceeds a user-specified value.  This time can then be
  printed into a special file as a useful diagnostic.
\end{itemize}

As an example:
\begin{lstlisting}
amr.grid_log = grdlog
amr.run_log = runlog 
\end{lstlisting}  
Every time the code regrids it prints a list of grids at all relevant
levels.  Here the code will write these grids lists into the file {\tt
  grdlog}.  Additionally, every time step the code prints certain
statements to the screen (if {\tt amr.v} = 1), such as:
\begin{verbatim}
STEP = 1 TIME = 1.91717746 DT = 1.91717746 
PLOTFILE: file = plt00001 
\end{verbatim}
The {\tt run\_log} option will output these statements into {\em
  runlog} as well.

Terser output can be obtained via:
\begin{lstlisting}
amr.run_log_terse} = runlogterse
\end{lstlisting}
This file, {\tt runlogterse} differs from {\tt runlog}, in that it
only contains lines of the form
\begin{verbatim}
10  0.2  0.005
\end{verbatim}
in which ``10'' is the number of steps taken, ``0.2'' is the
simulation time, and ``0.005'' is the level-0 time step.  This file
can be plotted very easily to monitor the time step.



\subsection{Other parameters}

There are a large number of solver-specific runtime parameters.  We describe these
together with the discussion of the physics solvers in later chapters.



% \section{Radiation}

% \begin{table*}[h]
% \begin{scriptsize}
% \begin{center}
% \begin{tabular}{|l|l|l|l|} \hline
% Parameter & Definition & Acceptable Values &Default\\
% \hline
% {\bf radiation.do\_timing} & & & \\
% {\bf radiation.do\_clouds} & & & \\
% {\bf radiation.do\_shadow} & & & \\
% {\bf radiation.do\_thermal\_wave} & & & \\
% {\bf radiation.do\_crooked\_pipe} & & & \\
% {\bf radiation.do\_linearMGD\_tp} & & & \\
% {\bf radiation.do\_neutrino\_test} & & & \\
% {\bf radiation.do\_rad\_sphere} & & & \\
% {\bf radiation.do\_light\_front} & & & \\
% {\bf radiation.do\_multigroup} & & & \\
% {\bf radiation.ngroups} & & & \\
% {\bf radiation.nNeutrinoSpecies} & & & \\
% {\bf radiation.nNeutrinoGroups} & & & \\
% {\bf radiation.plot\_neutrino\_group\_energies\_total} & & & \\
% \hline
% \end{tabular}
% \label{Table:Radiation}
% \end{center}
% \end{scriptsize}
% \end{table*}

% \begin{itemize}
% \item You must have USE\_RAD  = TRUE in the GNUmakefile for {\bf castro.do\_radiation} to be relevant.
% \end{itemize}

